{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0      37  https://insights.blackcoffer.com/ai-in-healthc...\n",
       "1      38  https://insights.blackcoffer.com/what-if-the-c...\n",
       "2      39  https://insights.blackcoffer.com/what-jobs-wil...\n",
       "3      40  https://insights.blackcoffer.com/will-machine-...\n",
       "4      41  https://insights.blackcoffer.com/will-ai-repla..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.read_excel('./Input.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "clean = re.compile('<.*?>')\n",
    "import traceback\n",
    "\n",
    "\n",
    "def get_data(df):\n",
    "    links = df['URL']\n",
    "    ids  = df['URL_ID']\n",
    "    j=0\n",
    "    text = []\n",
    "    for id in ids:\n",
    "        # print(i)\n",
    "        result = requests.get(links[j],headers={\n",
    "            'Accept': \"*/*\",\n",
    "            'Accept-Encoding': 'gzip, deflate, br',\n",
    "            'Connection': 'keep-alive',\n",
    "            'User-Agent' : 'User'\n",
    "            })\n",
    "        content = result.content\n",
    "        content = content.decode('utf-8').encode('cp850','replace').decode('cp850')\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        try:\n",
    "            header = soup.find('h1', class_= 'entry-title').string\n",
    "\n",
    "            try:\n",
    "                soup.find('pre', class_='wp-block-preformatted').decompose()\n",
    "            except:\n",
    "                pass\n",
    "            div = soup.find_all('div', {'class': 'td-post-content'})\n",
    "\n",
    "            divStr = str(div[0])\n",
    "            cleanText = re.sub(clean, '', divStr)\n",
    "            totalText = header + '. ' + cleanText\n",
    "            temp = re.sub('\\n|\\xa0|(|)|\"|,|','',totalText)\n",
    "            with open('./TextFiles/'+str(int(id)) +'.txt', 'w') as f:\n",
    "                f.write(temp)\n",
    "\n",
    "        except Exception:\n",
    "            traceback.print_exc()\n",
    "            print(j,'-->',id)\n",
    "            with open('./TextFiles/'+ str(int(id)) +'.txt', 'w') as f:\n",
    "                f.write(' ')\n",
    "        j=j+1\n",
    "    return text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_14704\\3184094393.py\", line 25, in get_data\n",
      "    header = soup.find('h1', class_= 'entry-title').string\n",
      "AttributeError: 'NoneType' object has no attribute 'string'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 --> 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_14704\\3184094393.py\", line 25, in get_data\n",
      "    header = soup.find('h1', class_= 'entry-title').string\n",
      "AttributeError: 'NoneType' object has no attribute 'string'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 --> 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_14704\\3184094393.py\", line 25, in get_data\n",
      "    header = soup.find('h1', class_= 'entry-title').string\n",
      "AttributeError: 'NoneType' object has no attribute 'string'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 --> 144\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', \"A'S\", 'ABLE', 'ABOUT', 'ABOVE', 'ACCORDING', 'ACCORDINGLY', 'ACROSS', 'ACTUALLY', 'AFTER', 'AFTERWARDS', 'AGAIN', 'AGAINST', \"AIN'T\", 'ALL', 'ALLOW', 'ALLOWS', 'ALMOST', 'ALONE', 'ALONG', 'ALREADY', 'ALSO', 'ALTHOUGH', 'ALWAYS', 'AM', 'AMONG', 'AMONGST', 'AN', 'AND', 'ANOTHER', 'ANY', 'ANYBODY', 'ANYHOW', 'ANYONE', 'ANYTHING', 'ANYWAY', 'ANYWAYS', 'ANYWHERE', 'APART', 'APPEAR', 'APPRECIATE', 'APPROPRIATE', 'ARE', \"AREN'T\", 'AROUND', 'AS', 'ASIDE', 'ASK', 'ASKING', 'ASSOCIATED', 'AT', 'AVAILABLE', 'AWAY', 'AWFULLY', 'B', 'BE', 'BECAME', 'BECAUSE', 'BECOME', 'BECOMES', 'BECOMING', 'BEEN', 'BEFORE', 'BEFOREHAND', 'BEHIND', 'BEING', 'BELIEVE', 'BELOW', 'BESIDE', 'BESIDES', 'BEST', 'BETTER', 'BETWEEN', 'BEYOND', 'BOTH', 'BRIEF', 'BUT', 'BY', 'C', \"C'MON\", \"C'S\", 'CAME', 'CAN', \"CAN'T\", 'CANNOT', 'CANT', 'CAUSE', 'CAUSES', 'CERTAIN', 'CERTAINLY', 'CHANGES', 'CLEARLY', 'CO', 'COM', 'COME', 'COMES', 'CONCERNING', 'CONSEQUENTLY', 'CONSIDER', 'CONSIDERING', 'CONTAIN', 'CONTAINING', 'CONTAINS', 'CORRESPONDING', 'COULD', \"COULDN'T\", 'COURSE', 'CURRENTLY', 'D', 'DEFINITELY', 'DESCRIBED', 'DESPITE', 'DID', \"DIDN'T\", 'DIFFERENT', 'DO', 'DOES', \"DOESN'T\", 'DOING', \"DON'T\", 'DONE', 'DOWN', 'DOWNWARDS', 'DURING', 'E', 'EACH', 'EDU', 'EG', 'EIGHT', 'EITHER', 'ELSE', 'ELSEWHERE', 'ENOUGH', 'ENTIRELY', 'ESPECIALLY', 'ET', 'ETC', 'EVEN', 'EVER', 'EVERY', 'EVERYBODY', 'EVERYONE', 'EVERYTHING', 'EVERYWHERE', 'EX', 'EXACTLY', 'EXAMPLE', 'EXCEPT', 'F', 'FAR', 'FEW', 'FIFTH', 'FIRST', 'FIVE', 'FOLLOWED', 'FOLLOWING', 'FOLLOWS', 'FOR', 'FORMER', 'FORMERLY', 'FORTH', 'FOUR', 'FROM', 'FURTHER', 'FURTHERMORE', 'G', 'GET', 'GETS', 'GETTING', 'GIVEN', 'GIVES', 'GO', 'GOES', 'GOING', 'GONE', 'GOT', 'GOTTEN', 'GREETINGS', 'H', 'HAD', \"HADN'T\", 'HAPPENS', 'HARDLY', 'HAS', \"HASN'T\", 'HAVE', \"HAVEN'T\", 'HAVING', 'HE', \"HE'S\", 'HELLO', 'HELP', 'HENCE', 'HER', 'HERE', \"HERE'S\", 'HEREAFTER', 'HEREBY', 'HEREIN', 'HEREUPON', 'HERS', 'HERSELF', 'HI', 'HIM', 'HIMSELF', 'HIS', 'HITHER', 'HOPEFULLY', 'HOW', 'HOWBEIT', 'HOWEVER', 'I', \"I'D\", \"I'LL\", \"I'M\", \"I'VE\", 'IE', 'IF', 'IGNORED', 'IMMEDIATE', 'IN', 'INASMUCH', 'INC', 'INDEED', 'INDICATE', 'INDICATED', 'INDICATES', 'INNER', 'INSOFAR', 'INSTEAD', 'INTO', 'INWARD', 'IS', \"ISN'T\", 'IT', \"IT'D\", \"IT'LL\", \"IT'S\", 'ITS', 'ITSELF', 'J', 'JUST', 'K', 'KEEP', 'KEEPS', 'KEPT', 'KNOW', 'KNOWS', 'KNOWN', 'L', 'LAST', 'LATELY', 'LATER', 'LATTER', 'LATTERLY', 'LEAST', 'LESS', 'LEST', 'LET', \"LET'S\", 'LIKE', 'LIKED', 'LIKELY', 'LITTLE', 'LOOK', 'LOOKING', 'LOOKS', 'LTD', 'M', 'MAINLY', 'MANY', 'MAY', 'MAYBE', 'ME', 'MEAN', 'MEANWHILE', 'MERELY', 'MIGHT', 'MORE', 'MOREOVER', 'MOST', 'MOSTLY', 'MUCH', 'MUST', 'MY', 'MYSELF', 'N', 'NAME', 'NAMELY', 'ND', 'NEAR', 'NEARLY', 'NECESSARY', 'NEED', 'NEEDS', 'NEITHER', 'NEVER', 'NEVERTHELESS', 'NEW', 'NEXT', 'NINE', 'NO', 'NOBODY', 'NON', 'NONE', 'NOONE', 'NOR', 'NORMALLY', 'NOT', 'NOTHING', 'NOVEL', 'NOW', 'NOWHERE', 'O', 'OBVIOUSLY', 'OF', 'OFF', 'OFTEN', 'OH', 'OK', 'OKAY', 'OLD', 'ON', 'ONCE', 'ONE', 'ONES', 'ONLY', 'ONTO', 'OR', 'OTHER', 'OTHERS', 'OTHERWISE', 'OUGHT', 'OUR', 'OURS', 'OURSELVES', 'OUT', 'OUTSIDE', 'OVER', 'OVERALL', 'OWN', 'P', 'PARTICULAR', 'PARTICULARLY', 'PER', 'PERHAPS', 'PLACED', 'PLEASE', 'PLUS', 'POSSIBLE', 'PRESUMABLY', 'PROBABLY', 'PROVIDES', 'Q', 'QUE', 'QUITE', 'QV', 'R', 'RATHER', 'RD', 'RE', 'REALLY', 'REASONABLY', 'REGARDING', 'REGARDLESS', 'REGARDS', 'RELATIVELY', 'RESPECTIVELY', 'RIGHT', 'S', 'SAID', 'SAME', 'SAW', 'SAY', 'SAYING', 'SAYS', 'SECOND', 'SECONDLY', 'SEE', 'SEEING', 'SEEM', 'SEEMED', 'SEEMING', 'SEEMS', 'SEEN', 'SELF', 'SELVES', 'SENSIBLE', 'SENT', 'SERIOUS', 'SERIOUSLY', 'SEVEN', 'SEVERAL', 'SHALL', 'SHE', 'SHOULD', \"SHOULDN'T\", 'SINCE', 'SIX', 'SO', 'SOME', 'SOMEBODY', 'SOMEHOW', 'SOMEONE', 'SOMETHING', 'SOMETIME', 'SOMETIMES', 'SOMEWHAT', 'SOMEWHERE', 'SOON', 'SORRY', 'SPECIFIED', 'SPECIFY', 'SPECIFYING', 'STILL', 'SUB', 'SUCH', 'SUP', 'SURE', 'T', \"T'S\", 'TAKE', 'TAKEN', 'TELL', 'TENDS', 'TH', 'THAN', 'THANK', 'THANKS', 'THANX', 'THAT', \"THAT'S\", 'THATS', 'THE', 'THEIR', 'THEIRS', 'THEM', 'THEMSELVES', 'THEN', 'THENCE', 'THERE', \"THERE'S\", 'THEREAFTER', 'THEREBY', 'THEREFORE', 'THEREIN', 'THERES', 'THEREUPON', 'THESE', 'THEY', \"THEY'D\", \"THEY'LL\", \"THEY'RE\", \"THEY'VE\", 'THINK', 'THIRD', 'THIS', 'THOROUGH', 'THOROUGHLY', 'THOSE', 'THOUGH', 'THREE', 'THROUGH', 'THROUGHOUT', 'THRU', 'THUS', 'TO', 'TOGETHER', 'TOO', 'TOOK', 'TOWARD', 'TOWARDS', 'TRIED', 'TRIES', 'TRULY', 'TRY', 'TRYING', 'TWICE', 'TWO', 'U', 'UN', 'UNDER', 'UNFORTUNATELY', 'UNLESS', 'UNLIKELY', 'UNTIL', 'UNTO', 'UP', 'UPON', 'US', 'USE', 'USED', 'USEFUL', 'USES', 'USING', 'USUALLY', 'UUCP', 'V', 'VALUE', 'VARIOUS', 'VERY', 'VIA', 'VIZ', 'VS', 'W', 'WANT', 'WANTS', 'WAS', \"WASN'T\", 'WAY', 'WE', \"WE'D\", \"WE'LL\", \"WE'RE\", \"WE'VE\", 'WELCOME', 'WELL', 'WENT', 'WERE', \"WEREN'T\", 'WHAT', \"WHAT'S\", 'WHATEVER', 'WHEN', 'WHENCE', 'WHENEVER', 'WHERE', \"WHERE'S\", 'WHEREAFTER', 'WHEREAS', 'WHEREBY', 'WHEREIN', 'WHEREUPON', 'WHEREVER', 'WHETHER', 'WHICH', 'WHILE', 'WHITHER', 'WHO', \"WHO'S\", 'WHOEVER', 'WHOLE', 'WHOM', 'WHOSE', 'WHY', 'WILL', 'WILLING', 'WISH', 'WITH', 'WITHIN', 'WITHOUT', \"WON'T\", 'WONDER', 'WOULD', 'WOULD', \"WOULDN'T\", 'X', 'Y', 'YES', 'YET', 'YOU', \"YOU'D\", \"YOU'LL\", \"YOU'RE\", \"YOU'VE\", 'YOUR', 'YOURS', 'YOURSELF', 'YOURSELVES', 'Z', 'ZERO']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ERNST',\n",
       " 'YOUNG',\n",
       " 'DELOITTE',\n",
       " 'TOUCHE',\n",
       " 'KPMG',\n",
       " 'PRICEWATERHOUSECOOPERS',\n",
       " 'PRICEWATERHOUSE',\n",
       " 'COOPERS',\n",
       " '',\n",
       " 'AFGHANI',\n",
       " 'ARIARY',\n",
       " 'BAHT',\n",
       " 'BALBOA',\n",
       " 'BIRR',\n",
       " 'BOLIVAR',\n",
       " 'BOLIVIANO',\n",
       " 'CEDI',\n",
       " 'COLON',\n",
       " 'CÃ“RDOBA',\n",
       " 'DALASI',\n",
       " 'DENAR',\n",
       " 'DINAR',\n",
       " 'DIRHAM',\n",
       " 'DOBRA',\n",
       " 'DONG',\n",
       " 'DRAM',\n",
       " 'ESCUDO',\n",
       " 'EURO',\n",
       " 'FLORIN',\n",
       " 'FORINT',\n",
       " 'GOURDE',\n",
       " 'GUARANI',\n",
       " 'GULDEN',\n",
       " 'HRYVNIA',\n",
       " 'KINA',\n",
       " 'KIP',\n",
       " 'KONVERTIBILNA MARKA',\n",
       " 'KORUNA',\n",
       " 'KRONA',\n",
       " 'KRONE',\n",
       " 'KROON',\n",
       " 'KUNA',\n",
       " 'KWACHA',\n",
       " 'KWANZA',\n",
       " 'KYAT',\n",
       " 'LARI',\n",
       " 'LATS',\n",
       " 'LEK',\n",
       " 'LEMPIRA',\n",
       " 'LEONE',\n",
       " 'LEU',\n",
       " 'LEV',\n",
       " 'LILANGENI',\n",
       " 'LIRA',\n",
       " 'LITAS',\n",
       " 'LOTI',\n",
       " 'MANAT',\n",
       " 'METICAL',\n",
       " 'NAIRA',\n",
       " 'NAKFA',\n",
       " 'NEW LIRA',\n",
       " 'NEW SHEQEL',\n",
       " 'NGULTRUM',\n",
       " 'NUEVO SOL',\n",
       " 'OUGUIYA',\n",
       " 'PATACA',\n",
       " 'PESO',\n",
       " 'POUND',\n",
       " 'PULA',\n",
       " 'QUETZAL',\n",
       " 'RAND',\n",
       " 'REAL',\n",
       " 'RENMINBI',\n",
       " 'RIAL',\n",
       " 'RIEL',\n",
       " 'RINGGIT',\n",
       " 'RIYAL',\n",
       " 'RUBLE',\n",
       " 'RUFIYAA',\n",
       " 'RUPEE',\n",
       " 'RUPEE',\n",
       " 'RUPIAH',\n",
       " 'SHILLING',\n",
       " 'SOM',\n",
       " 'SOMONI',\n",
       " 'SPECIAL DRAWING RIGHTS',\n",
       " 'TAKA',\n",
       " 'TALA',\n",
       " 'TENGE',\n",
       " 'TUGRIK',\n",
       " 'VATU',\n",
       " 'WON',\n",
       " 'YEN',\n",
       " 'ZLOTY',\n",
       " '',\n",
       " 'HUNDRED',\n",
       " 'Denominations',\n",
       " 'THOUSAND',\n",
       " 'MILLION',\n",
       " 'BILLION',\n",
       " 'TRILLION',\n",
       " 'DATE',\n",
       " 'Time related',\n",
       " 'ANNUAL',\n",
       " 'ANNUALLY',\n",
       " 'ANNUM',\n",
       " 'YEAR',\n",
       " 'YEARLY',\n",
       " 'QUARTER',\n",
       " 'QUARTERLY',\n",
       " 'QTR',\n",
       " 'MONTH',\n",
       " 'MONTHLY',\n",
       " 'WEEK',\n",
       " 'WEEKLY',\n",
       " 'DAY',\n",
       " 'DAILY',\n",
       " 'JANUARY',\n",
       " 'Calendar',\n",
       " 'FEBRUARY',\n",
       " 'MARCH',\n",
       " 'APRIL',\n",
       " 'MAY',\n",
       " 'JUNE',\n",
       " 'JULY',\n",
       " 'AUGUST',\n",
       " 'SEPTEMBER',\n",
       " 'OCTOBER',\n",
       " 'NOVEMBER',\n",
       " 'DECEMBER',\n",
       " 'JAN',\n",
       " 'FEB',\n",
       " 'MAR',\n",
       " 'APR',\n",
       " 'MAY',\n",
       " 'JUN',\n",
       " 'JUL',\n",
       " 'AUG',\n",
       " 'SEP',\n",
       " 'SEPT',\n",
       " 'OCT',\n",
       " 'NOV',\n",
       " 'DEC',\n",
       " 'MONDAY',\n",
       " 'TUESDAY',\n",
       " 'WEDNESDAY',\n",
       " 'THURSDAY',\n",
       " 'FRIDAY',\n",
       " 'SATURDAY',\n",
       " 'SUNDAY',\n",
       " 'ONE',\n",
       " 'Numbers',\n",
       " 'TWO',\n",
       " 'THREE',\n",
       " 'FOUR',\n",
       " 'FIVE',\n",
       " 'SIX',\n",
       " 'SEVEN',\n",
       " 'EIGHT',\n",
       " 'NINE',\n",
       " 'TEN',\n",
       " 'ELEVEN',\n",
       " 'TWELVE',\n",
       " 'THIRTEEN',\n",
       " 'FOURTEEN',\n",
       " 'FIFTEEN',\n",
       " 'SIXTEEN',\n",
       " 'SEVENTEEN',\n",
       " 'EIGHTEEN',\n",
       " 'NINETEEN',\n",
       " 'TWENTY',\n",
       " 'THIRTY',\n",
       " 'FORTY',\n",
       " 'FIFTY',\n",
       " 'SIXTY',\n",
       " 'SEVENTY',\n",
       " 'EIGHTY',\n",
       " 'NINETY',\n",
       " 'FIRST',\n",
       " 'SECOND',\n",
       " 'THIRD',\n",
       " 'FOURTH',\n",
       " 'FIFTH',\n",
       " 'SIXTH',\n",
       " 'SEVENTH',\n",
       " 'EIGHTH',\n",
       " 'NINTH',\n",
       " 'TENTH',\n",
       " 'I',\n",
       " 'Roman numerals',\n",
       " 'II',\n",
       " 'III',\n",
       " 'IV',\n",
       " 'V',\n",
       " 'VI',\n",
       " 'VII',\n",
       " 'VIII',\n",
       " 'IX',\n",
       " 'X',\n",
       " 'XI',\n",
       " 'XII',\n",
       " 'XIII',\n",
       " 'XIV',\n",
       " 'XV',\n",
       " 'XVI',\n",
       " 'XVII',\n",
       " 'XVIII',\n",
       " 'XIX',\n",
       " 'XX',\n",
       " 'ABOUT',\n",
       " 'ABOVE',\n",
       " 'AFTER',\n",
       " 'AGAIN',\n",
       " 'ALL',\n",
       " 'AM',\n",
       " 'AMONG',\n",
       " 'AN',\n",
       " 'AND',\n",
       " 'ANY',\n",
       " 'ARE',\n",
       " 'AS',\n",
       " 'AT',\n",
       " 'BE',\n",
       " 'BECAUSE',\n",
       " 'BEEN',\n",
       " 'BEFORE',\n",
       " 'BEING',\n",
       " 'BELOW',\n",
       " 'BETWEEN',\n",
       " 'BOTH',\n",
       " 'BUT',\n",
       " 'BY',\n",
       " 'CAN',\n",
       " 'DID',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " 'DOING',\n",
       " 'DOWN',\n",
       " 'DURING',\n",
       " 'EACH',\n",
       " 'FEW',\n",
       " 'FOR',\n",
       " 'FROM',\n",
       " 'FURTHER',\n",
       " 'HAD',\n",
       " 'HAS',\n",
       " 'HAVE',\n",
       " 'HAVING',\n",
       " 'HE',\n",
       " 'HER',\n",
       " 'HERE',\n",
       " 'HERS',\n",
       " 'HERSELF',\n",
       " 'HIM',\n",
       " 'HIMSELF',\n",
       " 'HIS',\n",
       " 'HOW',\n",
       " 'IF',\n",
       " 'IN',\n",
       " 'INTO',\n",
       " 'IS',\n",
       " 'IT',\n",
       " 'ITS',\n",
       " 'ITSELF',\n",
       " 'JUST',\n",
       " 'ME',\n",
       " 'MORE',\n",
       " 'MOST',\n",
       " 'MY',\n",
       " 'MYSELF',\n",
       " 'NO',\n",
       " 'NOR',\n",
       " 'NOT',\n",
       " 'NOW',\n",
       " 'OF',\n",
       " 'OFF',\n",
       " 'ON',\n",
       " 'ONCE',\n",
       " 'ONLY',\n",
       " 'OR',\n",
       " 'OTHER',\n",
       " 'OUR',\n",
       " 'OURS',\n",
       " 'OURSELVES',\n",
       " 'OUT',\n",
       " 'OVER',\n",
       " 'OWN',\n",
       " 'SAME',\n",
       " 'SHE',\n",
       " 'SHOULD',\n",
       " 'SO',\n",
       " 'SOME',\n",
       " 'SUCH',\n",
       " 'THAN',\n",
       " 'THAT',\n",
       " 'THE',\n",
       " 'THEIR',\n",
       " 'THEIRS',\n",
       " 'THEM',\n",
       " 'THEMSELVES',\n",
       " 'THEN',\n",
       " 'THERE',\n",
       " 'THESE',\n",
       " 'THEY',\n",
       " 'THIS',\n",
       " 'THOSE',\n",
       " 'THROUGH',\n",
       " 'TO',\n",
       " 'TOO',\n",
       " 'UNDER',\n",
       " 'UNTIL',\n",
       " 'UP',\n",
       " 'VERY',\n",
       " 'WAS',\n",
       " 'WE',\n",
       " 'WERE',\n",
       " 'WHAT',\n",
       " 'WHEN',\n",
       " 'WHERE',\n",
       " 'WHICH',\n",
       " 'WHILE',\n",
       " 'WHO',\n",
       " 'WHOM',\n",
       " 'WHY',\n",
       " 'WITH',\n",
       " 'YOU',\n",
       " 'YOUR',\n",
       " 'YOURS',\n",
       " 'YOURSELF',\n",
       " 'YOURSELVES',\n",
       " 'A',\n",
       " \"A'S\",\n",
       " 'ABLE',\n",
       " 'ABOUT',\n",
       " 'ABOVE',\n",
       " 'ACCORDING',\n",
       " 'ACCORDINGLY',\n",
       " 'ACROSS',\n",
       " 'ACTUALLY',\n",
       " 'AFTER',\n",
       " 'AFTERWARDS',\n",
       " 'AGAIN',\n",
       " 'AGAINST',\n",
       " \"AIN'T\",\n",
       " 'ALL',\n",
       " 'ALLOW',\n",
       " 'ALLOWS',\n",
       " 'ALMOST',\n",
       " 'ALONE',\n",
       " 'ALONG',\n",
       " 'ALREADY',\n",
       " 'ALSO',\n",
       " 'ALTHOUGH',\n",
       " 'ALWAYS',\n",
       " 'AM',\n",
       " 'AMONG',\n",
       " 'AMONGST',\n",
       " 'AN',\n",
       " 'AND',\n",
       " 'ANOTHER',\n",
       " 'ANY',\n",
       " 'ANYBODY',\n",
       " 'ANYHOW',\n",
       " 'ANYONE',\n",
       " 'ANYTHING',\n",
       " 'ANYWAY',\n",
       " 'ANYWAYS',\n",
       " 'ANYWHERE',\n",
       " 'APART',\n",
       " 'APPEAR',\n",
       " 'APPRECIATE',\n",
       " 'APPROPRIATE',\n",
       " 'ARE',\n",
       " \"AREN'T\",\n",
       " 'AROUND',\n",
       " 'AS',\n",
       " 'ASIDE',\n",
       " 'ASK',\n",
       " 'ASKING',\n",
       " 'ASSOCIATED',\n",
       " 'AT',\n",
       " 'AVAILABLE',\n",
       " 'AWAY',\n",
       " 'AWFULLY',\n",
       " 'B',\n",
       " 'BE',\n",
       " 'BECAME',\n",
       " 'BECAUSE',\n",
       " 'BECOME',\n",
       " 'BECOMES',\n",
       " 'BECOMING',\n",
       " 'BEEN',\n",
       " 'BEFORE',\n",
       " 'BEFOREHAND',\n",
       " 'BEHIND',\n",
       " 'BEING',\n",
       " 'BELIEVE',\n",
       " 'BELOW',\n",
       " 'BESIDE',\n",
       " 'BESIDES',\n",
       " 'BEST',\n",
       " 'BETTER',\n",
       " 'BETWEEN',\n",
       " 'BEYOND',\n",
       " 'BOTH',\n",
       " 'BRIEF',\n",
       " 'BUT',\n",
       " 'BY',\n",
       " 'C',\n",
       " \"C'MON\",\n",
       " \"C'S\",\n",
       " 'CAME',\n",
       " 'CAN',\n",
       " \"CAN'T\",\n",
       " 'CANNOT',\n",
       " 'CANT',\n",
       " 'CAUSE',\n",
       " 'CAUSES',\n",
       " 'CERTAIN',\n",
       " 'CERTAINLY',\n",
       " 'CHANGES',\n",
       " 'CLEARLY',\n",
       " 'CO',\n",
       " 'COM',\n",
       " 'COME',\n",
       " 'COMES',\n",
       " 'CONCERNING',\n",
       " 'CONSEQUENTLY',\n",
       " 'CONSIDER',\n",
       " 'CONSIDERING',\n",
       " 'CONTAIN',\n",
       " 'CONTAINING',\n",
       " 'CONTAINS',\n",
       " 'CORRESPONDING',\n",
       " 'COULD',\n",
       " \"COULDN'T\",\n",
       " 'COURSE',\n",
       " 'CURRENTLY',\n",
       " 'D',\n",
       " 'DEFINITELY',\n",
       " 'DESCRIBED',\n",
       " 'DESPITE',\n",
       " 'DID',\n",
       " \"DIDN'T\",\n",
       " 'DIFFERENT',\n",
       " 'DO',\n",
       " 'DOES',\n",
       " \"DOESN'T\",\n",
       " 'DOING',\n",
       " \"DON'T\",\n",
       " 'DONE',\n",
       " 'DOWN',\n",
       " 'DOWNWARDS',\n",
       " 'DURING',\n",
       " 'E',\n",
       " 'EACH',\n",
       " 'EDU',\n",
       " 'EG',\n",
       " 'EIGHT',\n",
       " 'EITHER',\n",
       " 'ELSE',\n",
       " 'ELSEWHERE',\n",
       " 'ENOUGH',\n",
       " 'ENTIRELY',\n",
       " 'ESPECIALLY',\n",
       " 'ET',\n",
       " 'ETC',\n",
       " 'EVEN',\n",
       " 'EVER',\n",
       " 'EVERY',\n",
       " 'EVERYBODY',\n",
       " 'EVERYONE',\n",
       " 'EVERYTHING',\n",
       " 'EVERYWHERE',\n",
       " 'EX',\n",
       " 'EXACTLY',\n",
       " 'EXAMPLE',\n",
       " 'EXCEPT',\n",
       " 'F',\n",
       " 'FAR',\n",
       " 'FEW',\n",
       " 'FIFTH',\n",
       " 'FIRST',\n",
       " 'FIVE',\n",
       " 'FOLLOWED',\n",
       " 'FOLLOWING',\n",
       " 'FOLLOWS',\n",
       " 'FOR',\n",
       " 'FORMER',\n",
       " 'FORMERLY',\n",
       " 'FORTH',\n",
       " 'FOUR',\n",
       " 'FROM',\n",
       " 'FURTHER',\n",
       " 'FURTHERMORE',\n",
       " 'G',\n",
       " 'GET',\n",
       " 'GETS',\n",
       " 'GETTING',\n",
       " 'GIVEN',\n",
       " 'GIVES',\n",
       " 'GO',\n",
       " 'GOES',\n",
       " 'GOING',\n",
       " 'GONE',\n",
       " 'GOT',\n",
       " 'GOTTEN',\n",
       " 'GREETINGS',\n",
       " 'H',\n",
       " 'HAD',\n",
       " \"HADN'T\",\n",
       " 'HAPPENS',\n",
       " 'HARDLY',\n",
       " 'HAS',\n",
       " \"HASN'T\",\n",
       " 'HAVE',\n",
       " \"HAVEN'T\",\n",
       " 'HAVING',\n",
       " 'HE',\n",
       " \"HE'S\",\n",
       " 'HELLO',\n",
       " 'HELP',\n",
       " 'HENCE',\n",
       " 'HER',\n",
       " 'HERE',\n",
       " \"HERE'S\",\n",
       " 'HEREAFTER',\n",
       " 'HEREBY',\n",
       " 'HEREIN',\n",
       " 'HEREUPON',\n",
       " 'HERS',\n",
       " 'HERSELF',\n",
       " 'HI',\n",
       " 'HIM',\n",
       " 'HIMSELF',\n",
       " 'HIS',\n",
       " 'HITHER',\n",
       " 'HOPEFULLY',\n",
       " 'HOW',\n",
       " 'HOWBEIT',\n",
       " 'HOWEVER',\n",
       " 'I',\n",
       " \"I'D\",\n",
       " \"I'LL\",\n",
       " \"I'M\",\n",
       " \"I'VE\",\n",
       " 'IE',\n",
       " 'IF',\n",
       " 'IGNORED',\n",
       " 'IMMEDIATE',\n",
       " 'IN',\n",
       " 'INASMUCH',\n",
       " 'INC',\n",
       " 'INDEED',\n",
       " 'INDICATE',\n",
       " 'INDICATED',\n",
       " 'INDICATES',\n",
       " 'INNER',\n",
       " 'INSOFAR',\n",
       " 'INSTEAD',\n",
       " 'INTO',\n",
       " 'INWARD',\n",
       " 'IS',\n",
       " \"ISN'T\",\n",
       " 'IT',\n",
       " \"IT'D\",\n",
       " \"IT'LL\",\n",
       " \"IT'S\",\n",
       " 'ITS',\n",
       " 'ITSELF',\n",
       " 'J',\n",
       " 'JUST',\n",
       " 'K',\n",
       " 'KEEP',\n",
       " 'KEEPS',\n",
       " 'KEPT',\n",
       " 'KNOW',\n",
       " 'KNOWS',\n",
       " 'KNOWN',\n",
       " 'L',\n",
       " 'LAST',\n",
       " 'LATELY',\n",
       " 'LATER',\n",
       " 'LATTER',\n",
       " 'LATTERLY',\n",
       " 'LEAST',\n",
       " 'LESS',\n",
       " 'LEST',\n",
       " 'LET',\n",
       " \"LET'S\",\n",
       " 'LIKE',\n",
       " 'LIKED',\n",
       " 'LIKELY',\n",
       " 'LITTLE',\n",
       " 'LOOK',\n",
       " 'LOOKING',\n",
       " 'LOOKS',\n",
       " 'LTD',\n",
       " 'M',\n",
       " 'MAINLY',\n",
       " 'MANY',\n",
       " 'MAY',\n",
       " 'MAYBE',\n",
       " 'ME',\n",
       " 'MEAN',\n",
       " 'MEANWHILE',\n",
       " 'MERELY',\n",
       " 'MIGHT',\n",
       " 'MORE',\n",
       " 'MOREOVER',\n",
       " 'MOST',\n",
       " 'MOSTLY',\n",
       " 'MUCH',\n",
       " 'MUST',\n",
       " 'MY',\n",
       " 'MYSELF',\n",
       " 'N',\n",
       " 'NAME',\n",
       " 'NAMELY',\n",
       " 'ND',\n",
       " 'NEAR',\n",
       " 'NEARLY',\n",
       " 'NECESSARY',\n",
       " 'NEED',\n",
       " 'NEEDS',\n",
       " 'NEITHER',\n",
       " 'NEVER',\n",
       " 'NEVERTHELESS',\n",
       " 'NEW',\n",
       " 'NEXT',\n",
       " 'NINE',\n",
       " 'NO',\n",
       " 'NOBODY',\n",
       " 'NON',\n",
       " 'NONE',\n",
       " 'NOONE',\n",
       " 'NOR',\n",
       " 'NORMALLY',\n",
       " 'NOT',\n",
       " 'NOTHING',\n",
       " 'NOVEL',\n",
       " 'NOW',\n",
       " 'NOWHERE',\n",
       " 'O',\n",
       " 'OBVIOUSLY',\n",
       " 'OF',\n",
       " 'OFF',\n",
       " 'OFTEN',\n",
       " 'OH',\n",
       " 'OK',\n",
       " 'OKAY',\n",
       " 'OLD',\n",
       " 'ON',\n",
       " 'ONCE',\n",
       " 'ONE',\n",
       " 'ONES',\n",
       " 'ONLY',\n",
       " 'ONTO',\n",
       " 'OR',\n",
       " 'OTHER',\n",
       " 'OTHERS',\n",
       " 'OTHERWISE',\n",
       " 'OUGHT',\n",
       " 'OUR',\n",
       " 'OURS',\n",
       " 'OURSELVES',\n",
       " 'OUT',\n",
       " 'OUTSIDE',\n",
       " 'OVER',\n",
       " 'OVERALL',\n",
       " 'OWN',\n",
       " 'P',\n",
       " 'PARTICULAR',\n",
       " 'PARTICULARLY',\n",
       " 'PER',\n",
       " 'PERHAPS',\n",
       " 'PLACED',\n",
       " 'PLEASE',\n",
       " 'PLUS',\n",
       " 'POSSIBLE',\n",
       " 'PRESUMABLY',\n",
       " 'PROBABLY',\n",
       " 'PROVIDES',\n",
       " 'Q',\n",
       " 'QUE',\n",
       " 'QUITE',\n",
       " 'QV',\n",
       " 'R',\n",
       " 'RATHER',\n",
       " 'RD',\n",
       " 'RE',\n",
       " 'REALLY',\n",
       " 'REASONABLY',\n",
       " 'REGARDING',\n",
       " 'REGARDLESS',\n",
       " 'REGARDS',\n",
       " 'RELATIVELY',\n",
       " 'RESPECTIVELY',\n",
       " 'RIGHT',\n",
       " 'S',\n",
       " 'SAID',\n",
       " 'SAME',\n",
       " 'SAW',\n",
       " 'SAY',\n",
       " 'SAYING',\n",
       " 'SAYS',\n",
       " 'SECOND',\n",
       " 'SECONDLY',\n",
       " 'SEE',\n",
       " 'SEEING',\n",
       " 'SEEM',\n",
       " 'SEEMED',\n",
       " 'SEEMING',\n",
       " 'SEEMS',\n",
       " 'SEEN',\n",
       " 'SELF',\n",
       " 'SELVES',\n",
       " 'SENSIBLE',\n",
       " 'SENT',\n",
       " 'SERIOUS',\n",
       " 'SERIOUSLY',\n",
       " 'SEVEN',\n",
       " 'SEVERAL',\n",
       " 'SHALL',\n",
       " 'SHE',\n",
       " 'SHOULD',\n",
       " \"SHOULDN'T\",\n",
       " 'SINCE',\n",
       " 'SIX',\n",
       " 'SO',\n",
       " 'SOME',\n",
       " 'SOMEBODY',\n",
       " 'SOMEHOW',\n",
       " 'SOMEONE',\n",
       " 'SOMETHING',\n",
       " 'SOMETIME',\n",
       " 'SOMETIMES',\n",
       " 'SOMEWHAT',\n",
       " 'SOMEWHERE',\n",
       " 'SOON',\n",
       " 'SORRY',\n",
       " 'SPECIFIED',\n",
       " 'SPECIFY',\n",
       " 'SPECIFYING',\n",
       " 'STILL',\n",
       " 'SUB',\n",
       " 'SUCH',\n",
       " 'SUP',\n",
       " 'SURE',\n",
       " 'T',\n",
       " \"T'S\",\n",
       " 'TAKE',\n",
       " 'TAKEN',\n",
       " 'TELL',\n",
       " 'TENDS',\n",
       " 'TH',\n",
       " 'THAN',\n",
       " 'THANK',\n",
       " 'THANKS',\n",
       " 'THANX',\n",
       " 'THAT',\n",
       " \"THAT'S\",\n",
       " 'THATS',\n",
       " 'THE',\n",
       " 'THEIR',\n",
       " 'THEIRS',\n",
       " 'THEM',\n",
       " 'THEMSELVES',\n",
       " 'THEN',\n",
       " 'THENCE',\n",
       " 'THERE',\n",
       " \"THERE'S\",\n",
       " 'THEREAFTER',\n",
       " 'THEREBY',\n",
       " 'THEREFORE',\n",
       " 'THEREIN',\n",
       " 'THERES',\n",
       " 'THEREUPON',\n",
       " 'THESE',\n",
       " 'THEY',\n",
       " \"THEY'D\",\n",
       " \"THEY'LL\",\n",
       " \"THEY'RE\",\n",
       " \"THEY'VE\",\n",
       " 'THINK',\n",
       " 'THIRD',\n",
       " 'THIS',\n",
       " 'THOROUGH',\n",
       " 'THOROUGHLY',\n",
       " 'THOSE',\n",
       " 'THOUGH',\n",
       " 'THREE',\n",
       " 'THROUGH',\n",
       " 'THROUGHOUT',\n",
       " 'THRU',\n",
       " 'THUS',\n",
       " 'TO',\n",
       " 'TOGETHER',\n",
       " 'TOO',\n",
       " 'TOOK',\n",
       " 'TOWARD',\n",
       " 'TOWARDS',\n",
       " 'TRIED',\n",
       " 'TRIES',\n",
       " 'TRULY',\n",
       " 'TRY',\n",
       " 'TRYING',\n",
       " 'TWICE',\n",
       " 'TWO',\n",
       " 'U',\n",
       " 'UN',\n",
       " 'UNDER',\n",
       " 'UNFORTUNATELY',\n",
       " 'UNLESS',\n",
       " 'UNLIKELY',\n",
       " 'UNTIL',\n",
       " 'UNTO',\n",
       " 'UP',\n",
       " 'UPON',\n",
       " 'US',\n",
       " 'USE',\n",
       " 'USED',\n",
       " 'USEFUL',\n",
       " 'USES',\n",
       " 'USING',\n",
       " 'USUALLY',\n",
       " 'UUCP',\n",
       " 'V',\n",
       " 'VALUE',\n",
       " 'VARIOUS',\n",
       " 'VERY',\n",
       " 'VIA',\n",
       " 'VIZ',\n",
       " 'VS',\n",
       " 'W',\n",
       " 'WANT',\n",
       " 'WANTS',\n",
       " 'WAS',\n",
       " \"WASN'T\",\n",
       " 'WAY',\n",
       " 'WE',\n",
       " \"WE'D\",\n",
       " \"WE'LL\",\n",
       " \"WE'RE\",\n",
       " \"WE'VE\",\n",
       " 'WELCOME',\n",
       " 'WELL',\n",
       " 'WENT',\n",
       " 'WERE',\n",
       " \"WEREN'T\",\n",
       " 'WHAT',\n",
       " \"WHAT'S\",\n",
       " 'WHATEVER',\n",
       " 'WHEN',\n",
       " 'WHENCE',\n",
       " 'WHENEVER',\n",
       " 'WHERE',\n",
       " \"WHERE'S\",\n",
       " 'WHEREAFTER',\n",
       " 'WHEREAS',\n",
       " 'WHEREBY',\n",
       " 'WHEREIN',\n",
       " 'WHEREUPON',\n",
       " 'WHEREVER',\n",
       " 'WHETHER',\n",
       " 'WHICH',\n",
       " 'WHILE',\n",
       " 'WHITHER',\n",
       " 'WHO',\n",
       " \"WHO'S\",\n",
       " 'WHOEVER',\n",
       " 'WHOLE',\n",
       " 'WHOM',\n",
       " 'WHOSE',\n",
       " 'WHY',\n",
       " 'WILL',\n",
       " 'WILLING',\n",
       " 'WISH',\n",
       " 'WITH',\n",
       " 'WITHIN',\n",
       " 'WITHOUT',\n",
       " \"WON'T\",\n",
       " 'WONDER',\n",
       " 'WOULD',\n",
       " 'WOULD',\n",
       " \"WOULDN'T\",\n",
       " 'X',\n",
       " 'Y',\n",
       " 'YES',\n",
       " 'YET',\n",
       " 'YOU',\n",
       " \"YOU'D\",\n",
       " \"YOU'LL\",\n",
       " \"YOU'RE\",\n",
       " \"YOU'VE\",\n",
       " 'YOUR',\n",
       " 'YOURS',\n",
       " 'YOURSELF',\n",
       " 'YOURSELVES',\n",
       " 'Z',\n",
       " 'ZERO',\n",
       " 'UNITED',\n",
       " 'STATE',\n",
       " 'NORTH',\n",
       " 'SOUTH',\n",
       " 'EAST',\n",
       " 'NORTHEAST',\n",
       " 'NORTHWEST',\n",
       " 'SOUTHEAST',\n",
       " 'SOUTHWEST',\n",
       " 'WEST',\n",
       " 'OCEAN',\n",
       " 'SEA',\n",
       " 'LAKE',\n",
       " 'RIVER',\n",
       " 'CREEK',\n",
       " 'GULF',\n",
       " 'MOUNTAIN',\n",
       " 'STREET',\n",
       " 'BOULEVARD',\n",
       " 'BLVD',\n",
       " 'PARKWAY',\n",
       " 'CITY',\n",
       " 'COUNTY',\n",
       " 'COUNTRY',\n",
       " 'PACIFIC',\n",
       " 'ATLANTIC',\n",
       " 'INDIAN',\n",
       " 'MEDITERRANEAN',\n",
       " 'COMMONWEALTH',\n",
       " 'AMERICA',\n",
       " 'AMERICAN',\n",
       " 'YORK',\n",
       " 'CHICAGO',\n",
       " 'LAS',\n",
       " 'VEGAS',\n",
       " 'LOS',\n",
       " 'ANGELES',\n",
       " 'MILWAUKEE',\n",
       " 'SUNNYVALE',\n",
       " 'FREMONT',\n",
       " 'CINCINNATI',\n",
       " 'PHILADELPHIA',\n",
       " 'MIAMI',\n",
       " 'DALLAS',\n",
       " 'FORT',\n",
       " 'BOSTON',\n",
       " 'HOUSTON',\n",
       " 'WASHINGTON',\n",
       " 'ATLANTA',\n",
       " 'DETROIT',\n",
       " 'SAN',\n",
       " 'FRANSICO',\n",
       " 'PHOENIX',\n",
       " 'SEATTLE',\n",
       " 'DIEGO',\n",
       " 'MINNEAPOLIS',\n",
       " 'MEMPHIS',\n",
       " 'DENVER',\n",
       " 'ST',\n",
       " 'LOUIS',\n",
       " 'PITTSBURGH',\n",
       " 'MANHATTAN',\n",
       " 'HOLLYWOOD',\n",
       " 'COLUMBUS',\n",
       " 'INDIANAPOLIS',\n",
       " 'MUMBAI',\n",
       " 'KARACHI',\n",
       " 'ONTARIO',\n",
       " 'TORONTO',\n",
       " 'CAMBRIDGE',\n",
       " 'DELHI',\n",
       " 'SAO',\n",
       " 'PAULO',\n",
       " 'SHANGHAI',\n",
       " 'MOSCOW',\n",
       " 'SEOUL',\n",
       " 'ISTANBUL',\n",
       " 'TOKYO',\n",
       " 'JAKARTA',\n",
       " 'BEIJING',\n",
       " 'LONDON',\n",
       " 'LUXEMBOURG',\n",
       " 'SINGAPORE',\n",
       " 'REPUBLIC',\n",
       " 'CHINA',\n",
       " 'INDIA',\n",
       " 'INDONESIA',\n",
       " 'BRAZIL',\n",
       " 'BRAZILIAN',\n",
       " 'PAKISTAN',\n",
       " 'BANGLADESH',\n",
       " 'RUSSIA',\n",
       " 'NIGERIA',\n",
       " 'NOVA',\n",
       " 'SCOTIA',\n",
       " 'JAPAN',\n",
       " 'MALAYSIA',\n",
       " 'MEXICO',\n",
       " 'MEXICAN',\n",
       " ...]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "auditor=''\n",
    "currencies = ''\n",
    "dates= ''\n",
    "gen= ''\n",
    "genlong = ''\n",
    "geo = ''\n",
    "names = ''\n",
    "\n",
    "with open('./StopWords/StopWords_Auditor.txt', 'r') as f:\n",
    "    auditor = f.read()\n",
    "\n",
    "with open('./StopWords/StopWords_Currencies.txt', 'r') as f:\n",
    "    currencies = f.read()\n",
    "\n",
    "with open('./StopWords/StopWords_DatesandNumbers.txt', 'r') as f:\n",
    "    dates = f.read()\n",
    "\n",
    "with open('./StopWords/StopWords_Generic.txt', 'r') as f:\n",
    "    gen = f.read()\n",
    "\n",
    "with open('./StopWords/StopWords_GenericLong.txt', 'r') as f:\n",
    "    genlong = f.read()\n",
    "    \n",
    "\n",
    "with open('./StopWords/StopWords_Geographic.txt', 'r') as f:\n",
    "    geo = f.read()\n",
    "\n",
    "with open('./StopWords/StopWords_Names.txt', 'r') as f:\n",
    "    names = f.read()\n",
    "\n",
    "auditor= auditor.split('\\n')\n",
    "c =currencies.split('\\n')\n",
    "temp = []\n",
    "for i in c:\n",
    "    j =i.split('|')\n",
    "    temp.append(j[0].strip())\n",
    "temp\n",
    "currencies = temp\n",
    "currencies[:-1]\n",
    "dates = dates.replace('\\n', '*').replace('|', '*')\n",
    "dates = dates.split('*')\n",
    "dates\n",
    "temp = []\n",
    "for d in dates:\n",
    "    j =d.split('|')\n",
    "    temp.append(j[0].strip())\n",
    "dates = temp\n",
    "dates\n",
    "gen = gen.split('\\n')\n",
    "gen\n",
    "\n",
    "genlong =  genlong.upper().split('\\n')\n",
    "print(genlong)\n",
    "geo = geo.split('\\n')\n",
    "geo\n",
    "temp = []\n",
    "for i in geo:\n",
    "    j =i.split('|')\n",
    "    temp.append(j[0].strip())\n",
    "geo = temp \n",
    "geo\n",
    "names  = names.split('\\n')\n",
    "temp = []\n",
    "for i in names:\n",
    "    j =i.split('|')\n",
    "    temp.append(j[0].strip())\n",
    "names = temp \n",
    "names\n",
    "stopwords = auditor + currencies + dates + gen + genlong + geo +names\n",
    "stopwords[:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.640418499096685\n",
      "4.799446746335098\n",
      "5.445747797392523\n",
      "4.87515225038054\n",
      "5.088435371265059\n",
      "5.04371165257384\n",
      "5.086021498540294\n",
      "0.0\n",
      "4.817567561057341\n",
      "4.905866300633986\n",
      "5.170054533877018\n",
      "5.585427131000479\n",
      "5.008764936247996\n",
      "4.833622180916452\n",
      "5.133293121150004\n",
      "5.458823518708189\n",
      "4.6999457380901\n",
      "5.279916747887704\n",
      "5.523224036170459\n",
      "5.435736660076061\n",
      "0.0\n",
      "5.077087790808645\n",
      "4.906882584471144\n",
      "4.908110880437315\n",
      "5.174298367541657\n",
      "4.8974358848783694\n",
      "4.919156410440108\n",
      "5.487804869125521\n",
      "5.194300504677978\n",
      "5.477706971479574\n",
      "4.665467620305194\n",
      "4.817204275176321\n",
      "5.724463514399601\n",
      "5.016605156795931\n",
      "4.833333328833023\n",
      "5.357101779553132\n",
      "4.738933027957472\n",
      "4.855593217047055\n",
      "4.596175472895191\n",
      "4.528393348664547\n",
      "4.675105475368976\n",
      "5.116049379557994\n",
      "4.746298119697633\n",
      "4.548245610045398\n",
      "4.988372088880089\n",
      "5.1639566360677795\n",
      "4.781103830887648\n",
      "4.854971497875256\n",
      "5.189610382870636\n",
      "5.343801648476196\n",
      "5.082278478088447\n",
      "4.980346819369842\n",
      "5.018570621824102\n",
      "5.5433639911016\n",
      "4.84615381747838\n",
      "4.935418763940041\n",
      "4.926605501928437\n",
      "5.065509515640811\n",
      "5.017806930629984\n",
      "5.273428883531737\n",
      "4.990862939095571\n",
      "4.852216744784715\n",
      "4.939910309470932\n",
      "5.322932913164639\n",
      "4.6217616340841365\n",
      "4.818671449893472\n",
      "5.090332797004227\n",
      "5.21098517418397\n",
      "4.6888686088605205\n",
      "5.5224999861937505\n",
      "4.764320777799802\n",
      "5.253295663603299\n",
      "4.245541832310642\n",
      "5.2154432743418075\n",
      "5.287090553673322\n",
      "5.179584116087349\n",
      "5.039001556131824\n",
      "4.819723615062987\n",
      "5.00965147184469\n",
      "4.202447158840437\n",
      "5.239407858479118\n",
      "4.541085267797608\n",
      "5.4069767284680905\n",
      "5.171832267469539\n",
      "4.47704714362466\n",
      "5.057343472757552\n",
      "5.221652418677121\n",
      "5.212787210183422\n",
      "5.167477036646419\n",
      "4.980392151118348\n",
      "5.166361971257548\n",
      "4.8239366915688064\n",
      "5.503012039905102\n",
      "5.323299214134076\n",
      "5.161380075035403\n",
      "5.3884785779996465\n",
      "6.149532703894566\n",
      "5.6155844009984826\n",
      "4.535816605914565\n",
      "4.639999989082353\n",
      "4.93277308851776\n",
      "4.992424223513544\n",
      "5.530546614902658\n",
      "4.943289220280445\n",
      "4.9537512795953225\n",
      "5.442964675481781\n",
      "5.035312993522772\n",
      "0.0\n",
      "5.479329602816391\n",
      "5.336216210447334\n",
      "5.08265497490128\n",
      "5.113967433492744\n",
      "5.897694515997558\n",
      "5.144379839976375\n"
     ]
    }
   ],
   "source": [
    "import textstat\n",
    "import pyphen\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import nltk\n",
    "#nltk.download('all')\n",
    "dic = pyphen.Pyphen(lang= 'en')\n",
    "positive = open('./MasterDictionary/positive-words.txt', 'r').read()\n",
    "positive= positive.upper().split('\\n')\n",
    "negative = open('./MasterDictionary/negative-words.txt', 'r').read()\n",
    "neagtive =  negative.upper().split('\\n')\n",
    "\n",
    "POSITIVE_SCORE = []\n",
    "NEGATIVE_SCORE = []\n",
    "POLARITY_SCORE = []\n",
    "SUBJECTIVITY_SCORE = []\n",
    "AVG_SENTENCE_LENGTH = []\n",
    "PERCENTAGE_OF_COMPLEX_WORDS = []\n",
    "FOG_INDEX =[]\n",
    "AVG_NUMBER_OF_WORDS_PER_SENTENCE = []\n",
    "COMPLEX_WORD_COUNT = []\n",
    "WORD_COUNT =[]\n",
    "SYLLABLE_PER_WORD = []\n",
    "PERSONAL_PRONOUN = []\n",
    "AVG_WORD_LENGTH = []\n",
    "\n",
    "\n",
    "\n",
    "for id in df['URL_ID']:\n",
    "    text  =  open('./TextFiles/' + str(int(id))+'.txt').read()\n",
    "    text = text.upper()\n",
    "    sentances = sent_tokenize(text)\n",
    "    words = tokenizer.tokenize(text)\n",
    "    def filter_stopwords(word):\n",
    "        return True if word  not in stopwords else False\n",
    "\n",
    "    filtered_words  = tuple( filter(filter_stopwords,words))\n",
    "    filtered_words\n",
    "\n",
    "\n",
    "    p_count = 0\n",
    "    n_count = 0\n",
    "    t_count = len(filtered_words)\n",
    "    for word in filtered_words:\n",
    "        if word in positive:\n",
    "            p_count = p_count +1\n",
    "        if word in negative:\n",
    "            n_count =  n_count +1\n",
    "    #print(p_count,n_count, t_count)\n",
    "    POSITIVE_SCORE.append(p_count)\n",
    "    NEGATIVE_SCORE.append(n_count)\n",
    "\n",
    "    polarity_score =  (p_count - n_count) / (p_count + n_count+ 0.000001)\n",
    "    POLARITY_SCORE.append( polarity_score)\n",
    "    subjectivity_score =  (p_count + n_count) / (t_count+0.000001 )\n",
    "    subjectivity_score\n",
    "    SUBJECTIVITY_SCORE.append(subjectivity_score)\n",
    "    average_sen_len =  len(words)/ (len(sentances)+ 0.000001 )\n",
    "    AVG_SENTENCE_LENGTH.append( average_sen_len)\n",
    "    def com_filter(word):\n",
    "        temp = dic.inserted(word)\n",
    "        temp = temp.split('-')\n",
    "        return True if len(temp)>2  else False\n",
    "    cw = tuple( filter(com_filter,filtered_words))\n",
    "    COMPLEX_WORD_COUNT.append(len(cw))\n",
    "    pcw = len(cw) / (len(filtered_words) +0.000001)\n",
    "    PERCENTAGE_OF_COMPLEX_WORDS.append(pcw)\n",
    "\n",
    "    fog_index = 0.4 * (subjectivity_score+pcw)\n",
    "    FOG_INDEX.append(fog_index)\n",
    "    WORD_COUNT.append(len(filtered_words))\n",
    "    char_count =  textstat.char_count(text, ignore_spaces=True)\n",
    "    char_count\n",
    "    sly_count = 0\n",
    "    for w in filtered_words:\n",
    "        temp = dic.inserted(w)\n",
    "        temp = temp.split('-')\n",
    "        sly_count  = sly_count + len(temp)\n",
    "    (sly_count/(len(filtered_words) +0.000001))\n",
    "    SYLLABLE_PER_WORD.append((sly_count/(len(filtered_words) +0.000001)))\n",
    "    avg_word_length = (char_count/(len(words) +0.000001))\n",
    "    avg_word_length\n",
    "    awps = len(words) / ( len(sentances) + 0.000001)\n",
    "    AVG_NUMBER_OF_WORDS_PER_SENTENCE.append(awps)\n",
    "    AVG_WORD_LENGTH.append(avg_word_length)\n",
    "    print(avg_word_length)\n",
    "    pronounRegex = re.compile(r'I|we|my|ours|us',re.I)\n",
    "    pronouns = pronounRegex.findall(text)\n",
    "    PERSONAL_PRONOUN.append(len(pronouns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 114 114 114 114 114 114 114 114 114 114 114 114\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "len(POSITIVE_SCORE),\n",
    "len(NEGATIVE_SCORE),\n",
    "len(POLARITY_SCORE),\n",
    "len(SUBJECTIVITY_SCORE),\n",
    "len(AVG_SENTENCE_LENGTH),\n",
    "len(PERCENTAGE_OF_COMPLEX_WORDS),\n",
    "len(FOG_INDEX),\n",
    "len(AVG_NUMBER_OF_WORDS_PER_SENTENCE),\n",
    "len(COMPLEX_WORD_COUNT),\n",
    "len(WORD_COUNT),\n",
    "len(SYLLABLE_PER_WORD) ,\n",
    "len(PERSONAL_PRONOUN),\n",
    "len(AVG_WORD_LENGTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_14704\\1211149633.py:17: FutureWarning: save is not part of the public API, usage can give unexpected results and will be removed in a future version\n",
      "  writer.save()\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "df['POSITIVE SCORE'] = np.array(POSITIVE_SCORE).tolist()\n",
    "df['NEGATIVE SCORE'] = np.array(NEGATIVE_SCORE).tolist()\n",
    "df['POLARITY SCORE'] = np.array(POLARITY_SCORE).tolist()\n",
    "df['SUBJECTIVITY SCORE'] = np.array(SUBJECTIVITY_SCORE).tolist()\n",
    "df['AVG SENTENCE LENGTH'] = np.array(AVG_SENTENCE_LENGTH).tolist()\n",
    "df['PERCENTAGE OF COMPLEX WORDS'] = np.array(PERCENTAGE_OF_COMPLEX_WORDS).tolist()\n",
    "df['FOG INDEX'] = np.array(FOG_INDEX).tolist()\n",
    "df['AVG NUMBER OF WORDS PER SENTENCE'] = np.array(AVG_NUMBER_OF_WORDS_PER_SENTENCE).tolist()\n",
    "df['COMPLEX WORD COUNT'] = np.array(COMPLEX_WORD_COUNT).tolist()\n",
    "df['WORD COUNT'] = np.array(WORD_COUNT).tolist()\n",
    "df['SYLLABLE PER WORD'] = np.array(SYLLABLE_PER_WORD).tolist()\n",
    "df['PERSONAL PRONOUNS']  =np.array(PERSONAL_PRONOUN).tolist()\n",
    "df['AVG WORD LENGTH'] = np.array(AVG_WORD_LENGTH).tolist()\n",
    "writer = pd.ExcelWriter(\"Output.xlsx\", engine='xlsxwriter')\n",
    "df.to_excel(writer,sheet_name = 'TextAnalysis', index=False)\n",
    "writer.save() \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39fd2d51b54a73b511c66a7cabcdd7debbf2d14b926d758da4f152fca9aa295a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
